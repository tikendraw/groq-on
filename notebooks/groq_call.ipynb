{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from groq import Groq\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to convert dates to years\n",
    "def days_to_years_func(days: int = 0) -> float:\n",
    "    \"\"\"converts days into years\"\"\"\n",
    "    return days / 365\n",
    "\n",
    "\n",
    "def calculate(expression):\n",
    "    \"\"\"Evaluate a mathematical expression\"\"\"\n",
    "    try:\n",
    "        result = eval(expression)\n",
    "        return json.dumps({\"result\": result})\n",
    "    except:  # noqa\n",
    "        return json.dumps({\"error\": \"Invalid expression\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Groq(\n",
    "    api_key=os.environ[\"GROQ_API_KEY\"],\n",
    ")\n",
    "\n",
    "client2 = Groq(\n",
    "    api_key=\"hh\",\n",
    "    base_url=\"http://localhost:8000/\",\n",
    ")\n",
    "MODEL = \"llama3-groq-70b-8192-tool-use-preview\"\n",
    "# MODEL='llama3-8b-8192'\n",
    "# MODEL='llama-3.1-70b-versatile'\n",
    "\n",
    "\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"days_to_years_func\",\n",
    "            \"description\": \"converts days to years\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"day\": {\n",
    "                        \"type\": \"int\",\n",
    "                        \"description\": \"number of days, e.g. 245\",\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"days\"],\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_stream(stream=False, client=client, prompt=None, printt=True):\n",
    "    print(\"stream: \", stream)\n",
    "\n",
    "    if not prompt:\n",
    "        random_words = [\n",
    "            \"potato\",\n",
    "            \"tomato\",\n",
    "            \"banana\",\n",
    "            \"rail\",\n",
    "            \"railed\",\n",
    "            \"nutt\",\n",
    "            \"bolt\",\n",
    "            \"go\",\n",
    "            \"Elephant\",\n",
    "        ]\n",
    "        word = random.sample(random_words, 1)[0]\n",
    "        prompt = \"spell \" + word\n",
    "\n",
    "    print(f\"prompt: {prompt}\")\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        model=MODEL,\n",
    "        stream=stream,\n",
    "    )\n",
    "\n",
    "    if printt:\n",
    "        if stream:\n",
    "            for chunk in chat_completion:\n",
    "                if chunk.choices[0].delta.content is not None:\n",
    "                    print(chunk.choices[0].delta.content, end=\"\")\n",
    "        else:\n",
    "            print(chat_completion.choices[0].message.content)\n",
    "    else:\n",
    "        return chat_completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stream:  True\n",
      "prompt: spell bolt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[40m\u001b[32m respnse in cnnnectin pool <Response [200] content self.content stream <httpcore._sync.http11.HTTP11ConnectionByteStream object at 0x72524c0c2010>>\u001b[0m\n",
      "\u001b[40m\u001b[31m respnse in cnnnectin pool steam <httpcore._sync.http11.HTTP11ConnectionByteStream object at 0x72524c0c2010>\u001b[0m\n",
      "B-O-L-T"
     ]
    }
   ],
   "source": [
    "call_stream(stream=True, client=client, printt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stream:  False\n",
      "prompt: spell tomato\n",
      "\u001b[40m\u001b[32m respnse in cnnnectin pool <Response [200] content self.content stream <httpcore._sync.http11.HTTP11ConnectionByteStream object at 0x72524c0d4e90>>\u001b[0m\n",
      "\u001b[40m\u001b[31m respnse in cnnnectin pool steam <httpcore._sync.http11.HTTP11ConnectionByteStream object at 0x72524c0d4e90>\u001b[0m\n",
      "t-o-m-a-t-o\n"
     ]
    }
   ],
   "source": [
    "call_stream(False, client, printt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stream:  False\n",
      "prompt: spell potato\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[40m\u001b[32m respnse in cnnnectin pool <Response [200] content self.content stream <httpcore._sync.http11.HTTP11ConnectionByteStream object at 0x72524c0dff50>>\u001b[0m\n",
      "\u001b[40m\u001b[31m respnse in cnnnectin pool steam <httpcore._sync.http11.HTTP11ConnectionByteStream object at 0x72524c0dff50>\u001b[0m\n",
      "p-o-t-a-t-o\n"
     ]
    }
   ],
   "source": [
    "out = call_stream(False, client2, printt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stream:  True\n",
      "prompt: spell banana\n",
      "\u001b[40m\u001b[32m respnse in cnnnectin pool <Response [200] content self.content stream <httpcore._sync.http11.HTTP11ConnectionByteStream object at 0x72524c099750>>\u001b[0m\n",
      "\u001b[40m\u001b[31m respnse in cnnnectin pool steam <httpcore._sync.http11.HTTP11ConnectionByteStream object at 0x72524c099750>\u001b[0m\n",
      "B-A-N-A-N-A"
     ]
    }
   ],
   "source": [
    "out2 = call_stream(True, client2, printt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_conversation(user_prompt, client=client):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a calculator assistant. Use the calculate function to perform mathematical operations and provide the results.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": user_prompt,\n",
    "        },\n",
    "    ]\n",
    "    tools = [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"calculate\",\n",
    "                \"description\": \"Evaluate a mathematical expression\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"expression\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The mathematical expression to evaluate\",\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"expression\"],\n",
    "                },\n",
    "            },\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"days_to_years_func\",\n",
    "                \"description\": \"converts days to years\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"day\": {\n",
    "                            \"type\": \"int\",\n",
    "                            \"description\": \"number of days, e.g. 245\",\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"days\"],\n",
    "                },\n",
    "            },\n",
    "        },\n",
    "    ]\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=messages,\n",
    "        tools=tools,\n",
    "        tool_choice=\"auto\",\n",
    "        max_tokens=500,\n",
    "        temperature=0.88,\n",
    "        top_p=0.77,\n",
    "        stream=False,\n",
    "    )\n",
    "    print(response)\n",
    "    response_message = response.choices[0].message\n",
    "    tool_calls = response_message.tool_calls\n",
    "    if tool_calls:\n",
    "        available_functions = {\n",
    "            \"calculate\": calculate,\n",
    "            \"days_to_years_func\": days_to_years_func,\n",
    "        }\n",
    "\n",
    "        messages.append(response_message)\n",
    "        for tool_call in tool_calls:\n",
    "            function_name = tool_call.function.name\n",
    "            function_to_call = available_functions[function_name]\n",
    "            function_args = json.loads(tool_call.function.arguments)\n",
    "            function_response = str(function_to_call(**function_args))\n",
    "            messages.append(\n",
    "                {\n",
    "                    \"tool_call_id\": tool_call.id,\n",
    "                    \"role\": \"tool\",\n",
    "                    \"name\": function_name,\n",
    "                    \"content\": function_response,\n",
    "                }\n",
    "            )\n",
    "        second_response = client.chat.completions.create(model=MODEL, messages=messages)\n",
    "        print(second_response)\n",
    "        return second_response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[40m\u001b[32m respnse in cnnnectin pool <Response [200] content self.content stream <httpcore._sync.http11.HTTP11ConnectionByteStream object at 0x72524c0d6210>>\u001b[0m\n",
      "\u001b[40m\u001b[31m respnse in cnnnectin pool steam <httpcore._sync.http11.HTTP11ConnectionByteStream object at 0x72524c0d6210>\u001b[0m\n",
      "ChatCompletion(id='chatcmpl-7ffabe54-9cd3-473f-8997-0a22d365bfa7', choices=[Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_t9ya', function=Function(arguments='{\"days\": 4312}', name='days_to_years_func'), type='function')]))], created=1722155525, model='llama3-groq-70b-8192-tool-use-preview', object='chat.completion', system_fingerprint='fp_ee4b521143', usage=CompletionUsage(completion_tokens=29, prompt_tokens=309, total_tokens=338, completion_time=0.089045641, prompt_time=0.022945398, queue_time=None, total_time=0.11199103899999999), x_groq={'id': 'req_01j3w7fsjhe76b897hmjvy90t6'})\n",
      "\u001b[40m\u001b[32m respnse in cnnnectin pool <Response [200] content self.content stream <httpcore._sync.http11.HTTP11ConnectionByteStream object at 0x72524c0ec050>>\u001b[0m\n",
      "\u001b[40m\u001b[31m respnse in cnnnectin pool steam <httpcore._sync.http11.HTTP11ConnectionByteStream object at 0x72524c0ec050>\u001b[0m\n",
      "ChatCompletion(id='chatcmpl-b1647bbe-e58a-47e4-9c4e-4d79e4c755eb', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"You've been running for approximately 11.81 years.\", role='assistant', function_call=None, tool_calls=None))], created=1722155526, model='llama3-groq-70b-8192-tool-use-preview', object='chat.completion', system_fingerprint='fp_ee4b521143', usage=CompletionUsage(completion_tokens=13, prompt_tokens=107, total_tokens=120, completion_time=0.038203567, prompt_time=0.009568831, queue_time=None, total_time=0.047772398), x_groq={'id': 'req_01j3w7ft54es5vwa1m76hhbwtr'})\n"
     ]
    }
   ],
   "source": [
    "# user_prompt = \"calculate 3*25-55*2/25-4-2?\"\n",
    "user_prompt = \"i have been running for  4312 days , what would that be in years\"\n",
    "response = run_conversation(user_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[40m\u001b[32m respnse in cnnnectin pool <Response [200] content self.content stream <httpcore._sync.http11.HTTP11ConnectionByteStream object at 0x725246b44150>>\u001b[0m\n",
      "\u001b[40m\u001b[31m respnse in cnnnectin pool steam <httpcore._sync.http11.HTTP11ConnectionByteStream object at 0x725246b44150>\u001b[0m\n",
      "ChatCompletion(id='chatcmpl-3bbbd52d-1819-40e1-9820-d74c9add04a8', choices=[Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_vnr1', function=Function(arguments='{\"days\": 4832}', name='days_to_years_func'), type='function')]))], created=1722155599, model='llama3-groq-70b-8192-tool-use-preview', object='chat.completion', system_fingerprint='fp_ee4b521143', usage=CompletionUsage(completion_tokens=29, prompt_tokens=309, total_tokens=338, completion_time=0.088948484, prompt_time=0.024025886, queue_time=None, total_time=0.11297436999999999), x_groq={'id': 'req_01j3w7j17qen9s6md09cg4jawd'})\n",
      "\u001b[40m\u001b[32m respnse in cnnnectin pool <Response [200] content self.content stream <httpcore._sync.http11.HTTP11ConnectionByteStream object at 0x725244a02b90>>\u001b[0m\n",
      "\u001b[40m\u001b[31m respnse in cnnnectin pool steam <httpcore._sync.http11.HTTP11ConnectionByteStream object at 0x725244a02b90>\u001b[0m\n",
      "ChatCompletion(id='chatcmpl-c9f308bf-d47a-413e-8c79-781fbfe45cbd', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='You have been running for approximately 13.24 years.', role='assistant', function_call=None, tool_calls=None))], created=1722155601, model='llama3-groq-70b-8192-tool-use-preview', object='chat.completion', system_fingerprint='fp_ee4b521143', usage=CompletionUsage(completion_tokens=13, prompt_tokens=82, total_tokens=95, completion_time=0.038152842, prompt_time=0.008239298, queue_time=None, total_time=0.04639214), x_groq={'id': 'req_01j3w7j37yetgb6xrfx3tjpp3t'})\n"
     ]
    }
   ],
   "source": [
    "# user_prompt = \"calculate 3*25-55*2/25-4-2?\"\n",
    "user_prompt = \"i have been running for  4832 days , what would that be in years\"\n",
    "response = run_conversation(user_prompt, client2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.923287671232877"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "days_to_years_func(4352)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64.6"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(3 * 25) - (55 * 2 / 25) - 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import Dict, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class B(BaseModel):\n",
    "    d: dict\n",
    "\n",
    "\n",
    "class A(BaseModel):\n",
    "    b: list[B]\n",
    "    c: Dict[str, B]\n",
    "    e: str | None = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = A(b=[B(d={\"hi\": \"by\"})], c={\"another\": B(d={\"no\": \"yes\"})})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = a.model_dump()\n",
    "aaa = a.model_dump_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'b': [{'d': {'hi': 'by'}}],\n",
       "  'c': {'another': {'d': {'no': 'yes'}}},\n",
       "  'e': None},\n",
       " '{\"b\":[{\"d\":{\"hi\":\"by\"}}],\"c\":{\"another\":{\"d\":{\"no\":\"yes\"}}},\"e\":null}')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa, aaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "A(b=[B(d={'hi': 'by'})], c={'another': B(d={'no': 'yes'})}, e=None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = A(**aa)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "A(b=[B(d={'hi': 'by'})], c={'another': B(d={'no': 'yes'})}, e=None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'b': [B(d={'hi': 'by'})], 'c': {'another': B(d={'no': 'yes'})}, 'e': None}\n",
      "b : [B(d={'hi': 'by'})]\n",
      "c : {'another': B(d={'no': 'yes'})}\n",
      "e : None\n",
      "{'b': [B(d={'hi': 'by'})], 'c': {'another': B(d={'no': 'yes'})}}\n"
     ]
    }
   ],
   "source": [
    "print(a.__dict__)\n",
    "\n",
    "to_del = []\n",
    "for i, j in a.__dict__.items():\n",
    "    if j is None:\n",
    "        to_del.append(i)\n",
    "\n",
    "    print(i, \":\", j)\n",
    "\n",
    "\n",
    "for i in to_del:\n",
    "    a.__delattr__(i)\n",
    "print(a.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "delattr(s, \"b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "A(c={'another': B(d={'no': 'yes'})}, e=None)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'c': {'another': {'d': {'no': 'yes'}}}, 'e': None}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "groqon-A4JUjgUu-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
